{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76c84c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0848614]]\n",
      "[[0.12137072]]\n",
      "[[0.0848617]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39468/1511108539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;34m\"\"\"Stochastic gradient descent prediction. Decided to use similar gamma as in gradient descent\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mw_SGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.994842503189421e-10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mgeneretate_csv_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midsTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_SGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtxTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"testSub_Stochastic_Gradient_Descent.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from Additional_Functions import compute_mse\n",
    "\n",
    "#code here\n",
    "y, txTemp, txTestTemp, idsTest = load_data()\n",
    "\n",
    "tx = build_poly(txTemp,1)\n",
    "txTest = build_poly(txTestTemp,1)\n",
    "\n",
    "#remember to output prediction\n",
    "\"\"\"Least squares prediction:\"\"\"\n",
    "w_ls, mse = least_squares(y, tx)\n",
    "generetate_csv_prediction(idsTest, w_ls, txTest, \"testSub_least_squares.csv\")\n",
    "print(mse)\n",
    "\n",
    "w_init = np.zeros(w_ls.shape)\n",
    "\n",
    "\"\"\"Gradien descent prediction. Got gamma with the help of cross validation, see additional functions\"\"\"\n",
    "w_GD, mse = least_squares_GD(y, tx, w_init, 10, 2.395026619987486e-07)\n",
    "generetate_csv_prediction(idsTest, w_GD, txTest, \"testSub_Gradient_Descent.csv\")\n",
    "print(mse)\n",
    "\n",
    "\"\"\"Ridge regresion prediction, lambda decided in same way as gamma above\"\"\"\n",
    "w_RR, mse = ridge_regression(y, tx, 3.290344562312671e-06)\n",
    "generetate_csv_prediction(idsTest, w_RR, txTest, \"testSub_Ridge_Regression.csv\")\n",
    "print(mse)\n",
    "\n",
    "\"\"\"Stochastic gradient descent prediction. Decided to use similar gamma as in gradient descent\"\"\"\n",
    "w_SGD, mse, log_loss = least_squares_SGD(np.squeeze(np.asarray(y)), tx, np.squeeze(np.asarray(w_init)), 500, 5.994842503189421e-10)\n",
    "generetate_csv_prediction(idsTest, w_SGD, txTest, \"testSub_Stochastic_Gradient_Descent.csv\")\n",
    "print(mse)\n",
    "print(log_loss)\n",
    "print(\"---\")\n",
    "\n",
    "\"\"\"Logistic regression prediction. Got gamma with the help of cross validation, see additional functions\"\"\"\n",
    "w_LR, log_loss = logistic_regression(y, tx, w_init, 10, 1e-15)\n",
    "mse = compute_mse(y, tx, w_LR)\n",
    "generetate_csv_prediction(idsTest, w_LR, txTest, \"testSub_Logistic_Regression.csv\")\n",
    "print(mse)\n",
    "print(log_loss)\n",
    "\n",
    "\"\"\"Regularized logistic regression prediction. Got lambda from cross validation and used same gamma as in logistic regression\"\"\"\n",
    "w_RLR, log_loss = reg_logistic_regression(y, tx, 1e5, w_init, 10, 1e-15)\n",
    "mse = compute_mse(y, tx, w_RLR)\n",
    "generetate_csv_prediction(idsTest, w_RLR, txTest, \"testSub_Regularized_Logistic_Regression.csv\")\n",
    "print(mse)\n",
    "print(log_loss)\n",
    "\n",
    "\"\"\"The best test result we got\"\"\"\n",
    "tx1 = build_poly(txTemp,5)\n",
    "txTest1 = build_poly(txTestTemp,5)\n",
    "w_Best, mse = ridge_regression(y, tx1, 0.11)\n",
    "generetate_csv_prediction(idsTest, w_Best, txTest1, \"testSub_Best.csv\")\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fa4482fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ferdi\\AppData\\Local\\Temp/ipykernel_39468/1307350728.py:51: RuntimeWarning: invalid value encountered in sqrt\n",
      "  loss_tr = np.sqrt(2 * compute_mse(train_y, train_x , weights))\n",
      "C:\\Users\\Ferdi\\AppData\\Local\\Temp/ipykernel_39468/1307350728.py:52: RuntimeWarning: invalid value encountered in sqrt\n",
      "  loss_te = np.sqrt(2 * compute_mse(test_y, test_x , weights))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000000000000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_log_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    txw = (tx @ w)\n",
    "    yxw = y * txw\n",
    "    return (np.log(1 + np.exp(txw)) - yxw).sum()\n",
    "\n",
    "def cross_validation_visualization(lambds, mse_tr, mse_te,name,degree):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx    (lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda^%s\"%degree)\n",
    "    plt.ylabel(\"rmse\")\n",
    "  \n",
    "    plt.title(\"cross validation \"+name)\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"CD_\"+name)\n",
    "\n",
    "def cross_validation(y, tx, degree):\n",
    "    #w_init, mse = least_squares(y, tx)\n",
    "    w_init = np.zeros((tx.shape[1],1))\n",
    "    #gradient descent -15 to -6.5 gives good overview gamma = 3.727593720314938e-07 with degree 5\n",
    "    #logistic gradeint diverges after around -14 graph until 13.8 gives good overview -14 good with rmse\n",
    "    lambdas = np.logspace(-9, -8.7, 3)\n",
    "    rmse_te = []\n",
    "    rmse_tr = []\n",
    "    min_loss1 = 100000000000000000\n",
    "    min_lambda = 0\n",
    "    for lambda_ in lambdas:\n",
    "        k_indices = build_k_indices(y,4,1)\n",
    "        tr = []\n",
    "        te = []\n",
    "        \n",
    "        for k in range(len(k_indices)):\n",
    "\n",
    "            #make a mask to extract all test data\n",
    "            mask = np.zeros(tx.shape[0], dtype=bool)\n",
    "            mask[k_indices[k]] = True\n",
    "\n",
    "            test_x = tx[mask,...]\n",
    "            test_y = y[mask]\n",
    "        \n",
    "            amask = np.invert(mask)\n",
    "            train_x = tx[amask,...]\n",
    "            train_y = y[amask]\n",
    "            \n",
    "            weights, loss, loss2 = least_squares_SGD(y, tx, w_init, 100, lambda_)\n",
    "\n",
    "            # calculate the loss for train and test data:\n",
    "            \"\"\"rmse loss\"\"\"\n",
    "            loss_tr = np.sqrt(2 * compute_mse(train_y, train_x , weights))\n",
    "            loss_te = np.sqrt(2 * compute_mse(test_y, test_x , weights))\n",
    "        \n",
    "            \"\"\"logistic loss\"\"\"\n",
    "            \n",
    "            #loss_tr = calculate_log_loss(train_y, train_x, weights)\n",
    "            #loss_te = calculate_log_loss(test_y, test_x, weights)\n",
    "            \"\"\"predictive loss\"\"\"\n",
    "            #e = train_y - predict_labels(weights,train_x)\n",
    "            #loss_tr = np.sqrt(1/len(e)*e.T @ e)\n",
    "            #e = test_y - predict_labels(weights,test_x)\n",
    "            #loss_te = np.sqrt(1/len(e)*e.T @ e)\n",
    "            tr.append(loss_tr)\n",
    "            te.append(loss_te)\n",
    "\n",
    "        rmse_tr.append(np.array(tr).mean())\n",
    "        rmse_te.append(np.array(te).mean())\n",
    "        \n",
    "        \n",
    "        if(rmse_te[-1]<min_loss1):\n",
    "            min_lambda = lambda_\n",
    "            min_loss1 = rmse_te[-1]\n",
    "        \n",
    "            \n",
    "    degreeStr = str(1)\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te, 'GD test degree = %s' % degree, degreeStr)\n",
    "    print(min_lambda)\n",
    "    print(min_loss1)\n",
    "    \n",
    "cross_validation(y, tx, 1)\n",
    "#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "65cb200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        #setup for computing gradient\n",
    "        gradient = np.empty(tx.shape[1])\n",
    "\n",
    "        #Using 1 as batch size\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 1):\n",
    "            \"\"\"Compute a gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "            #using derivative of mse\n",
    "            eff = (w * minibatch_tx).sum(axis=1)\n",
    "            e = minibatch_y - eff\n",
    "            \n",
    "            #make vector of e vector times sequenced columns of tx (because of the different derivatives sequence one tx each)\n",
    "            for j in range(minibatch_tx.shape[1]):\n",
    "                gradient[j] = (-(e * minibatch_tx[:,j]) * (1/(minibatch_tx.shape[0]))).sum()\n",
    "\n",
    "            #update weight by gradient\n",
    "            w = w - gamma * gradient\n",
    "    log_loss = (np.log(1 + np.exp(tx @ w)) - y*(tx @ w)).sum()\n",
    "    mse = compute_mse(y, tx, w)\n",
    "\n",
    "    return w, mse, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f61fd5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17802939989636413\n",
      "166035.25527634544\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Stochastic gradient descent prediction. Decided to use similar gamma as in gradient descent\"\"\"\n",
    "w_SGD, mse, log_loss = least_squares_SGD(np.squeeze(np.asarray(y)), tx, np.squeeze(np.asarray(w_init)), 500, 2.005026619987486e-07)\n",
    "generetate_csv_prediction(idsTest, w_SGD, txTest, \"testSub_Stochastic_Gradient_Descent.csv\")\n",
    "print(mse)\n",
    "print(log_loss)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bde4e17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11377155495926186\n",
      "177397.77392646036\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Stochastic gradient descent prediction. Decided to use similar gamma as in gradient descent\"\"\"\n",
    "w_SGD, mse, log_loss = least_squares_SGD(np.squeeze(np.asarray(y)), tx, np.squeeze(np.asarray(w_init)), 1000, 1.5e-08)\n",
    "generetate_csv_prediction(idsTest, w_SGD, txTest, \"testSub_Stochastic_Gradient_Descent.csv\")\n",
    "print(mse)\n",
    "print(log_loss)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "33385ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11671748388094622\n",
      "194759.99511429627\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Stochastic gradient descent prediction. Decided to use similar gamma as in gradient descent\"\"\"\n",
    "w_SGD, mse, log_loss = least_squares_SGD(np.squeeze(np.asarray(y)), tx, np.squeeze(np.asarray(w_init)), 800, 2.11e-08)\n",
    "generetate_csv_prediction(idsTest, w_SGD, txTest, \"testSub_Stochastic_Gradient_Descent.csv\")\n",
    "print(mse)\n",
    "print(log_loss)\n",
    "#171101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a29541ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from Additional_Functions import compute_mse\n",
    "\n",
    "#code here\n",
    "y, txTemp, txTestTemp, idsTest = load_data()\n",
    "\n",
    "tx = build_poly(txTemp,1)\n",
    "txTest = build_poly(txTestTemp,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e9a5e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0848614]]\n"
     ]
    }
   ],
   "source": [
    "#remember to output prediction\n",
    "\"\"\"Least squares prediction:\"\"\"\n",
    "w_ls, mse = least_squares(y, tx)\n",
    "generetate_csv_prediction(idsTest, w_ls, txTest, \"testSub_least_squares.csv\")\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f84fa26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.zeros(w_ls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86d5530d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10964683]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Gradien descent prediction. Got gamma with the help of cross validation, see additional functions\"\"\"\n",
    "w_GD, mse = least_squares_GD(y, tx, w_init, 25, 2.395026619987486e-07)\n",
    "generetate_csv_prediction(idsTest, w_GD, txTest, \"testSub_Gradient_Descent.csv\")\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6575ebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0848617]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Ridge regresion prediction, lambda decided in same way as gamma above\"\"\"\n",
    "w_RR, mse = ridge_regression(y, tx, 3.290344562312671e-06)\n",
    "generetate_csv_prediction(idsTest, w_RR, txTest, \"testSub_Ridge_Regression.csv\")\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e92438d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0063169703503623\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Stochastic gradient descent prediction. Decided to use similar gamma as in gradient descent\"\"\"\n",
    "w_SGD, mse = least_squares_SGD(np.squeeze(np.asarray(y)), tx, np.squeeze(np.asarray(w_init)), 25, 2.395026619987486e-07)\n",
    "generetate_csv_prediction(idsTest, w_SGD, txTest, \"testSub_Stochastic_Gradient_Descent.csv\")\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "353f9e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17279901]]\n",
      "173012.8326703143\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Logistic regression prediction. Got gamma with the help of cross validation, see additional functions\"\"\"\n",
    "w_LR, log_loss = logistic_regression(y, tx, w_init, 25, 1e-15)\n",
    "mse = compute_mse(y, tx, w_LR)\n",
    "generetate_csv_prediction(idsTest, w_LR, txTest, \"testSub_Logistic_Regression.csv\")\n",
    "print(mse)\n",
    "print(log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "970c6337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17279901]]\n",
      "173013.22204008835\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Regularized logistic regression prediction. Got lambda from cross validation and used same gamma as in logistic regression\"\"\"\n",
    "w_RLR, log_loss = reg_logistic_regression(y, tx, 1e5, w_init, 25, 1e-15)\n",
    "mse = compute_mse(y, tx, w_RLR)\n",
    "generetate_csv_prediction(idsTest, w_RLR, txTest, \"testSub_Regularized_Logistic_Regression.csv\")\n",
    "print(mse)\n",
    "print(log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da47632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07881887]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The best test result we got\"\"\"\n",
    "tx1 = build_poly(txTemp,5)\n",
    "txTest1 = build_poly(txTestTemp,5)\n",
    "w_Best, mse = ridge_regression(y, tx1, 0.11)\n",
    "generetate_csv_prediction(idsTest, w_Best, txTest1, \"testSub_Best.csv\")\n",
    "print(mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
